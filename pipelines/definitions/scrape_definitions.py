from pipelines.definitions.scrape_sources import scrape_free_dictionary


def scrape_definitions(idioms: list[dict], delay: int):
    """
    Collect definition scrape results without writing to the DB.

    idioms: [{'id': int, 'idiom': str}, ...]
    returns: [
        {'idiom_id': int, 'job': 'definition', 'content': str, 'status': 'success'|'not_found'|'error'|'http_XXX'},
        ...
    ]
    """
    results = []

    for row in idioms:
        idiom_id = row["id"]
        idiom_text = row["idiom"]

        print(f"ğŸ” Scraping: {idiom_text}")
        res = scrape_free_dictionary(idiom_text, delay)

        definition = (res.get("definition") or "").strip()
        status = res.get("status") or "error"

        # Build the staging payload shape
        results.append(
            {
                "idiom_id": idiom_id,
                "job": "definition",
                "content": definition,  # empty string if not_found/error
                "status": status,
            }
        )

        if status == "success" and definition:
            print(f"âœ… Got definition for: {idiom_text}")

    print(f"\nğŸ“Š scrape_definitions: {len(results)} results collected")
    return results
